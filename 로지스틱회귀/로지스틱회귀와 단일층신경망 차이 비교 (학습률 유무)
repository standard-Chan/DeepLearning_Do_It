import matplotlib.pyplot as plt
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.linear_model import SGDClassifier

#데이터 셋 분할

cancer = load_breast_cancer()
x = cancer.data
y = cancer.target

# 훈련:테스트  8:2 분할
x_train_all, x_test, y_train_all, y_test = train_test_split(x,y, stratify=y, test_size=0.2, random_state=42)
print(x_train_all.shape, x_test.shape, y_train_all.shape, y_test.shape)

# (훈련:검증):테스트  (6:2):2
x_train, x_val, y_train, y_val = train_test_split(x_train_all, y_train_all, stratify=y_train_all, test_size=0.2, random_state=42)

#표준화 구현
train_mean = np.mean(x_train, axis=0)  #평균
train_std = np.std(x_train, axis=0)    # 편차 , axis=0 은 1차원 계산
x_train_scaled = (x_train - train_mean) / train_std

#단일층 신경망
class LogisticUnit:
    def __init__(self, learning_rate=0.1):
        self.w = None
        self.b = None
        self.w_history = []
        self.lr = learning_rate
        
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b
        return z
    
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad
    
    def fit(self, x, y, epochs=100):
        self.w = np.ones(x.shape[1])
        self.b = 0
        self.w_history.append(self.w.copy())
        for i in range(epochs):
            for x_i, y_i in zip(x, y):
                z = self.forpass(x_i)
                a = self.activation(z)
                err = -(y_i - a)
                w_grad, b_grad = self.backprop(x_i, err)
                self.w -= w_grad
                self.b -= b_grad
                self.w_history.append(self.w.copy())
    
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))  #시그모이드 함수
        return a
    
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]
        a = self.activation(np.array(z))
        return a > 0.5
#로지스틱
class LogisticUnit:
    
    def __init__(self, learning_rate=0.1):
        self.w = None
        self.b = None
        self.w_history = []
        self.lr = learning_rate
        
    def forpass(self, x):
        z = np.sum(x * self.w) + self.b
        return z
    
    def backprop(self, x, err):
        w_grad = x * err
        b_grad = 1 * err
        return w_grad, b_grad
    
    def fit(self, x, y, epochs=100):
        self.w = np.ones(x.shape[1])
        self.b = 0
        self.w_history.append(self.w.copy())
        for i in range(epochs):
            for x_i, y_i in zip(x, y):
                z = self.forpass(x_i)
                a = self.activation(z)
                err = -(y_i - a)
                w_grad, b_grad = self.backprop(x_i, err)
                self.w -= w_grad
                self.b -= b_grad
                self.w_history.append(self.w.copy())
    
    def activation(self, z):
        z = np.clip(z, -100, None)
        a = 1 / (1 + np.exp(-z))  #시그모이드 함수
        return a
    
    def predict(self, x):
        z = [self.forpass(x_i) for x_i in x]
        a = self.activation(np.array(z))
        return a > 0.5

### 단층 신경망
layer1 = SingleLayer()
layer1.fit(x_train, y_train)
layer1.score(x_val, y_val)

## 가중치 확인
w2 = []
w3 = []
for w in layer1.w_history:
    w2.append(w[2])
    w3.append(w[3])

plt.plot(w2, w3)
plt.xlabel('w[2]')
plt.ylabel('w[3]')
plt.plot(w2[-1], w3[-1], 'ro')
plt.show()

## 표준화를 이용하여 스케일 조정후 가중치 확인

layer2 = SingleLayer()
layer2.fit(x_train_scaled, y_train)
w2=[]
w3=[]
for w in layer2.w_history:
    w2.append(w[2])
    w3.append(w[3])
plt.plot(w2, w3)
plt.plot(w2[-1], w3[-1], 'ro')
plt.show()


### 로지스틱 (학습률X)
## 가중치 확인
logistic = LogisticUnit()
logistic.fit(x_train, y_train)
print('정확도 : ', np.mean(logistic.predict(x_val) == y_val))

## 스케일 조정 후() 가중치 변화 확인
Lw2 = []
Lw3 = []
for w in logistic.w_history:
    Lw2.append(w[2])
    Lw3.append(w[3])

plt.plot(Lw2, Lw3)
plt.xlabel('Lw[2]')
plt.ylabel('Lw[3]')
plt.plot(Lw2[-1], Lw3[-1], 'ro')

lg2 = LogisticUnit()
lg2.fit(x_train_scaled, y_train)
lw2=[]
lw3=[]
for w in lg2.w_history:
    lw2.append(w[2])
    lw3.append(w[3])
plt.plot(lw2, lw3)
plt.plot(lw2[-1], lw3[-1], 'ro')
plt.show()
plt.show()


